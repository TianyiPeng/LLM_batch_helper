# LLM Batch Helper Examples

This directory contains examples demonstrating the key features of the `llm_batch_helper` package.

## Available Examples

### ğŸ“ `example.py` - Prompt Mode Features
Demonstrates traditional prompt-based processing with:
- Multiple input formats (string, tuple, dictionary)
- File-based prompt processing
- Custom verification callbacks
- Caching functionality
- Error handling and retry logic

### ğŸ’¬ `example_message.py` - Message Mode Features â­ NEW!
Demonstrates the new conversation-based processing with:
- Multi-turn conversation handling
- System, user, and assistant message roles
- Mixed conversation lengths in batches
- Dictionary and tuple message formats
- Conversation caching and verification
- Error handling for message validation

## Directory Structure

```
examples/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ example.py                  # Prompt mode examples
â”œâ”€â”€ example_message.py          # Message mode examples (NEW!)
â”œâ”€â”€ example_prompts/           # Sample prompt files
â”‚   â”œâ”€â”€ france.txt
â”‚   â”œâ”€â”€ math.txt
â”‚   â””â”€â”€ shakespeare.txt
```

## Setup and Running

### 1. Set up API Keys

For **OpenAI** (used in `example.py`):
```bash
export OPENAI_API_KEY="your-openai-api-key-here"
```

For **OpenRouter** (used in `example_message.py`):
```bash
export OPENROUTER_API_KEY="your-openrouter-api-key-here"
```

For **Other Providers**:
```bash
export TOGETHER_API_KEY="your-together-api-key"     # Together.ai
export GEMINI_API_KEY="your-gemini-api-key"         # Google Gemini
```

### 2. Run the Examples

**Traditional Prompt Mode:**
```bash
cd examples
python example.py
```

**New Message Mode:**
```bash
cd examples  
python example_message.py
```

## Example Features Demonstrated

### Prompt Mode (`example.py`)
- âœ… String prompts with auto-generated IDs
- âœ… Tuple format: `("custom_id", "prompt_text")`
- âœ… Dictionary format: `{"id": "custom_id", "text": "prompt_text"}`
- âœ… File-based processing from text files
- âœ… Custom verification callbacks
- âœ… Response caching and cache validation
- âœ… Force regeneration capabilities
- âœ… Error handling and retry mechanisms

### Message Mode (`example_message.py`)
- ğŸ’¬ Multi-turn conversations with context
- ğŸ­ System, user, and assistant message roles
- ğŸ“‹ Tuple format: `("message_id", [message_list])`
- ğŸ“‹ Dictionary format: `{"id": "message_id", "messages": [message_list]}`
- ğŸ”„ Mixed conversation lengths in single batch
- ğŸ’¾ Conversation-aware caching
- âœ… Message format validation
- ğŸš« Input mode conflict prevention

## Key Differences: Prompt vs Message Mode

| Feature | Prompt Mode | Message Mode |
|---------|-------------|--------------|
| **Input** | Single prompts | Multi-turn conversations |
| **Context** | System instruction + user prompt | Full conversation history |
| **Use Cases** | Q&A, independent tasks | Chat, tutorials, ongoing assistance |
| **Format** | `prompts=[...]` | `messages=[...]` |
| **Caching** | By prompt content | By conversation context |

## Customizing the Examples

### Adding Your Own Prompts
1. Create text files in `example_prompts/` directory
2. Each file should contain a single prompt
3. The filename (without `.txt`) becomes the prompt ID
4. Run `example.py` to process them

### Creating Your Own Conversations
1. Define conversations in the message format:
   ```python
   messages = [
       ("conversation_id", [
           {"role": "system", "content": "You are a helpful assistant."},
           {"role": "user", "content": "Hello!"},
           {"role": "assistant", "content": "Hi! How can I help?"},
           {"role": "user", "content": "Tell me about Python."}
       ])
   ]
   ```
2. Process with `process_prompts_batch(messages=messages, ...)`

### Force Cache Regeneration
```python
# Regenerate all responses
results = process_prompts_batch(
    prompts=your_prompts,  # or messages=your_messages
    config=config,
    force=True  # Ignore cache
)
```



Happy batch processing! ğŸš€ 
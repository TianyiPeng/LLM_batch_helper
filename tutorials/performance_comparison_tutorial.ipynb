{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Batch Helper: Performance Comparison Tutorial\n",
        "\n",
        "This notebook demonstrates the dramatic performance improvements you get when using LLM Batch Helper compared to naive sequential API calls.\n",
        "\n",
        "## What We'll Demonstrate\n",
        "\n",
        "1. **Generate 5K Test Prompts** - Create a large dataset for testing\n",
        "2. **Naive Approach** - Show how slow sequential API calls are with time estimation\n",
        "3. **Smart Start** - Begin with 10 examples to validate output quality\n",
        "4. **Medium Scale** - Process 2.5K prompts with 100 concurrent requests\n",
        "5. **Large Scale with Caching** - Process 5K prompts with 500 concurrent requests, leveraging cache\n",
        "\n",
        "## Key Benefits We'll See\n",
        "\n",
        "- **üöÄ Speed**: 10-100x faster than sequential processing\n",
        "- **üíæ Caching**: Resume interrupted work without losing progress\n",
        "- **‚öôÔ∏è Tunable Concurrency**: Adjust for rate limits and optimal performance\n",
        "- **üõ°Ô∏è Reliability**: Built-in retry logic and error handling\n",
        "\n",
        "Let's get started! üéØ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "%pip install llm_batch_helper openai tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ All packages imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "\n",
        "# Import LLM Batch Helper\n",
        "from llm_batch_helper import LLMConfig, process_prompts_batch\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API key is configured!\n",
            "üöÄ Ready to start the performance comparison!\n"
          ]
        }
      ],
      "source": [
        "# Check if OpenAI API key is set\n",
        "import os\n",
        "# set your OPENAI_API_KEY here\n",
        "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found!\")\n",
        "    print(\"Please set your API key: export OPENAI_API_KEY='your-api-key'\")\n",
        "    print(\"Or add it to a .env file in your project directory\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key is configured!\")\n",
        "    print(\"üöÄ Ready to start the performance comparison!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate 5K Test Prompts\n",
        "\n",
        "First, let's create a large dataset of simple test prompts to benchmark performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Generating test prompts...\n",
            "‚úÖ Generated 5,000 test prompts\n",
            "\n",
            "üìã Sample prompts:\n",
            "  1. test 1: just respond 1\n",
            "  2. test 2: just respond 2\n",
            "  3. test 3: just respond 3\n",
            "  4. test 4: just respond 4\n",
            "  5. test 5: just respond 5\n",
            "  ...\n",
            "  4998. test 4998: just respond 4998\n",
            "  4999. test 4999: just respond 4999\n",
            "  5000. test 5000: just respond 5000\n"
          ]
        }
      ],
      "source": [
        "# Generate 5K naive test prompts\n",
        "def generate_test_prompts(count=5000):\n",
        "    \"\"\"Generate simple test prompts for performance testing.\"\"\"\n",
        "    prompts = []\n",
        "    for i in range(1, count + 1):\n",
        "        prompts.append(f\"test {i}: just respond {i}\")\n",
        "    return prompts\n",
        "\n",
        "# Generate the prompts\n",
        "print(\"üìù Generating test prompts...\")\n",
        "test_prompts = generate_test_prompts(5000)\n",
        "\n",
        "print(f\"‚úÖ Generated {len(test_prompts):,} test prompts\")\n",
        "print(\"\\nüìã Sample prompts:\")\n",
        "for i in range(5):\n",
        "    print(f\"  {i+1}. {test_prompts[i]}\")\n",
        "print(\"  ...\")\n",
        "for i in range(-3, 0):\n",
        "    print(f\"  {len(test_prompts)+i+1}. {test_prompts[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Naive Approach - Sequential API Calls\n",
        "\n",
        "Let's see how long it would take to process these prompts using the traditional sequential approach with a simple for loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è  Testing naive approach with 3 sample prompts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Naive API calls:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:00<00:00,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sample 1: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Naive API calls:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:00<00:00,  2.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sample 2: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Naive API calls: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sample 3: 3\n",
            "\n",
            "üìä Naive Approach Performance:\n",
            "   ‚è±Ô∏è  Average time per request: 0.46 seconds\n",
            "   üìù Sample size: 3 prompts\n",
            "   üïê Time for sample: 1.39 seconds\n",
            "\n",
            "üö® Estimated time for 5,000 prompts:\n",
            "   ‚è≥ Total: 2310 seconds\n",
            "   ‚è≥ That's 38.5 minutes\n",
            "   ‚è≥ Or 0.6 hours!\n",
            " Sometime, one API call can take 10-30 seconds\n",
            "depend on the prompt lenght, the response length, and the model you are using.\n",
            " In that case, the total time can be 90.2 hours for finishing 5000 prompts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def naive_chat_completion(prompt, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"Naive function that makes a single ChatGPT API call.\"\"\"\n",
        "    client = openai.OpenAI()\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=1.0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def estimate_naive_approach_time(prompts, sample_size=5):\n",
        "    \"\"\"Estimate how long the naive approach would take by timing a small sample.\"\"\"\n",
        "    print(f\"‚è±Ô∏è  Testing naive approach with {sample_size} sample prompts...\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "    \n",
        "    for i, prompt in enumerate(tqdm(prompts[:sample_size], desc=\"Naive API calls\")):\n",
        "        try:\n",
        "            response = naive_chat_completion(prompt)\n",
        "            results.append({\"prompt\": prompt, \"response\": response})\n",
        "            print(f\"‚úÖ Sample {i+1}: {response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error on sample {i+1}: {e}\")\n",
        "            results.append({\"prompt\": prompt, \"error\": str(e)})\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_time_per_request = elapsed_time / sample_size\n",
        "    \n",
        "    # Estimate total time for all prompts\n",
        "    total_estimated_time = avg_time_per_request * len(prompts)\n",
        "    \n",
        "    print(f\"\\nüìä Naive Approach Performance:\")\n",
        "    print(f\"   ‚è±Ô∏è  Average time per request: {avg_time_per_request:.2f} seconds\")\n",
        "    print(f\"   üìù Sample size: {sample_size} prompts\")\n",
        "    print(f\"   üïê Time for sample: {elapsed_time:.2f} seconds\")\n",
        "    print(f\"\\nüö® Estimated time for {len(prompts):,} prompts:\")\n",
        "    print(f\"   ‚è≥ Total: {total_estimated_time:.0f} seconds\")\n",
        "    print(f\"   ‚è≥ That's {total_estimated_time/60:.1f} minutes\")\n",
        "    print(f\"   ‚è≥ Or {total_estimated_time/3600:.1f} hours!\")\n",
        "    \n",
        "    print(f\" Sometime, one API call can take 10-30 seconds\")\n",
        "    print(f\"depend on the prompt lenght, the response length, and the model you are using.\")\n",
        "    print(f\" In that case, the total time can be {30 / avg_time_per_request * len(prompts) / 3600:.1f} hours for finishing {len(prompts)} prompts.\")\n",
        "    return results, avg_time_per_request\n",
        "\n",
        "# Run the estimation (only if API key is available)\n",
        "if os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    naive_results, avg_time = estimate_naive_approach_time(test_prompts, sample_size=3)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Skipping naive approach test - no API key configured\")\n",
        "    print(\"üí° With typical API response times of 1-3 seconds per request:\")\n",
        "    print(f\"   üìä 5,000 prompts √ó 2 seconds = 10,000 seconds\")\n",
        "    print(f\"   ‚è≥ That's 2.8 hours of sequential processing!\")\n",
        "\n",
        "naive_estimated_time = avg_time * len(test_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Smart Start - Test with 10 Examples\n",
        "\n",
        "Before processing thousands of prompts, let's start small to validate our approach and check output quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è  Configuration created:\n",
            "   ü§ñ Model: gpt-4o-mini\n",
            "   üå°Ô∏è  Temperature: 1.0\n",
            "   üî¢ Max tokens: 50\n",
            "   üöÄ Max concurrent: 10\n",
            "   üîÑ Max retries: 3\n"
          ]
        }
      ],
      "source": [
        "# Create configuration for LLM Batch Helper\n",
        "config = LLMConfig(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=1.0,\n",
        "    max_completion_tokens=50,\n",
        "    max_concurrent_requests=10,  # Start conservative\n",
        "    max_retries=3\n",
        ")\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuration created:\")\n",
        "print(f\"   ü§ñ Model: {config.model_name}\")\n",
        "print(f\"   üå°Ô∏è  Temperature: {config.temperature}\")\n",
        "print(f\"   üî¢ Max tokens: {config.max_completion_tokens}\")\n",
        "print(f\"   üöÄ Max concurrent: {config.max_concurrent_requests}\")\n",
        "print(f\"   üîÑ Max retries: {config.max_retries}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing with first 10 prompts to validate output quality...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Small Scale Test (10 prompts): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 13.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚è±Ô∏è  Completed in 0.73 seconds\n",
            "üìä Average time per prompt: 0.07 seconds\n",
            "\n",
            "üîç Validating output quality:\n",
            "‚úÖ [NEW] 1\n",
            "‚úÖ [NEW] 2\n",
            "‚úÖ [NEW] 3\n",
            "‚úÖ [NEW] 4\n",
            "‚úÖ [NEW] 5\n",
            "‚úÖ [NEW] 6\n",
            "‚úÖ [NEW] 7\n",
            "‚úÖ [NEW] 8\n",
            "‚úÖ [NEW] 9\n",
            "‚úÖ [NEW] 10\n",
            "\n",
            "üìà Results: 10 successful (0 cached), 0 failed\n",
            "üìä Success rate: 100.0%\n",
            "üéâ All outputs look good! Ready to scale up.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test with first 10 prompts to validate output quality\n",
        "print(\"üß™ Testing with first 10 prompts to validate output quality...\")\n",
        "\n",
        "small_test_prompts = test_prompts[:10]\n",
        "\n",
        "start_time = time.time()\n",
        "small_results = process_prompts_batch(\n",
        "    config=config,\n",
        "    provider=\"openai\",\n",
        "    prompts=small_test_prompts,\n",
        "    cache_dir=\"performance_cache\",\n",
        "    desc=\"Small Scale Test (10 prompts)\"\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Completed in {elapsed_time:.2f} seconds\")\n",
        "print(f\"üìä Average time per prompt: {elapsed_time/len(small_test_prompts):.2f} seconds\")\n",
        "\n",
        "# Validate outputs\n",
        "print(\"\\nüîç Validating output quality:\")\n",
        "successful = 0\n",
        "cached = 0\n",
        "failed = 0\n",
        "\n",
        "for prompt_id, response in small_results.items():\n",
        "    if \"error\" in response:\n",
        "        status = \"‚ùå [ERROR]\"\n",
        "        print(f\"{status} {prompt_id}: {response['error'][:100]}...\")\n",
        "        failed += 1\n",
        "    elif response.get(\"from_cache\"):\n",
        "        status = \"üíæ [CACHE]\"\n",
        "        print(f\"{status} {response['response_text']}\")\n",
        "        cached += 1\n",
        "        successful += 1\n",
        "    else:\n",
        "        status = \"‚úÖ [NEW]\"\n",
        "        print(f\"{status} {response['response_text']}\")\n",
        "        successful += 1\n",
        "\n",
        "print(f\"\\nüìà Results: {successful} successful ({cached} cached), {failed} failed\")\n",
        "print(f\"üìä Success rate: {successful/len(small_test_prompts)*100:.1f}%\")\n",
        "\n",
        "if successful == len(small_test_prompts):\n",
        "    print(\"üéâ All outputs look good! Ready to scale up.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Some outputs failed. You might want to adjust the configuration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Medium Scale - 2.5K Examples with 100 Concurrent Requests\n",
        "\n",
        "Now let's scale up to 2,500 prompts with higher concurrency to see the real performance benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Processing 2,500 prompts with 100 concurrent requests...\n",
            "üí° This would take ~0.6 hours with naive approach!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Medium Scale (2,500 prompts): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [00:37<00:00, 65.86it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ Medium Scale Results:\n",
            "   ‚è±Ô∏è  Total time: 38.0 seconds (0.6 minutes)\n",
            "   üìä Average time per prompt: 0.015 seconds\n",
            "   ‚úÖ Successful: 2,500\n",
            "   üíæ From cache: 10\n",
            "   ‚ùå Failed: 0\n",
            "   üìà Success rate: 100.0%\n",
            "\n",
            "üöÄ Performance Improvement:\n",
            "   üìä Naive approach would take: 38.5 minutes\n",
            "   ‚ö° Our approach took: 0.6 minutes\n",
            "   üéØ Speedup: 60.8x faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Update configuration for medium scale processing\n",
        "medium_config = LLMConfig(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=1.0,\n",
        "    max_completion_tokens=50,\n",
        "    max_concurrent_requests=100,  # Increase concurrency\n",
        "    max_retries=3\n",
        ")\n",
        "\n",
        "# Process 2.5K prompts\n",
        "medium_prompts = test_prompts[:2500]\n",
        "\n",
        "print(f\"üöÄ Processing {len(medium_prompts):,} prompts with {medium_config.max_concurrent_requests} concurrent requests...\")\n",
        "print(f\"üí° This would take ~{naive_estimated_time / 3600:.1f} hours with naive approach!\")\n",
        "\n",
        "start_time = time.time()\n",
        "medium_results = process_prompts_batch(\n",
        "    config=medium_config,\n",
        "    provider=\"openai\",\n",
        "    prompts=medium_prompts,\n",
        "    cache_dir=\"performance_cache\",\n",
        "    desc=f\"Medium Scale ({len(medium_prompts):,} prompts)\"\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Analyze results\n",
        "successful = sum(1 for r in medium_results.values() if \"error\" not in r)\n",
        "cached = sum(1 for r in medium_results.values() if r.get(\"from_cache\", False))\n",
        "failed = len(medium_results) - successful\n",
        "\n",
        "print(f\"\\nüéØ Medium Scale Results:\")\n",
        "print(f\"   ‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
        "print(f\"   üìä Average time per prompt: {elapsed_time/len(medium_prompts):.3f} seconds\")\n",
        "print(f\"   ‚úÖ Successful: {successful:,}\")\n",
        "print(f\"   üíæ From cache: {cached:,}\")\n",
        "print(f\"   ‚ùå Failed: {failed:,}\")\n",
        "print(f\"   üìà Success rate: {successful/len(medium_prompts)*100:.1f}%\")\n",
        "\n",
        "# Calculate speedup vs naive approach\n",
        "speedup = naive_estimated_time / elapsed_time\n",
        "print(f\"\\nüöÄ Performance Improvement:\")\n",
        "print(f\"   üìä Naive approach would take: {naive_estimated_time/60:.1f} minutes\")\n",
        "print(f\"   ‚ö° Our approach took: {elapsed_time/60:.1f} minutes\")\n",
        "print(f\"   üéØ Speedup: {speedup:.1f}x faster!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Large Scale with Caching - 5K Examples\n",
        "\n",
        "Now for the grand finale! Let's process all 5K prompts with even higher concurrency. The first 2.5K should be served from cache, demonstrating the caching feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Processing ALL 5,000 prompts!\n",
            "üíæ First 2,500 should be served from cache\n",
            "üöÄ Only 2,500 new prompts need processing\n",
            "‚ö° Using 500 concurrent requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Large Scale (5,000 prompts): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:36<00:00, 137.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÜ FINAL RESULTS - Large Scale Processing:\n",
            "============================================================\n",
            "   üìä Total prompts: 5,000\n",
            "   ‚è±Ô∏è  Total time: 36.5 seconds (0.6 minutes)\n",
            "   üìà Average time per prompt: 0.0073 seconds\n",
            "\n",
            "üìã Response Breakdown:\n",
            "   ‚úÖ Successful: 5,000 (100.0%)\n",
            "   üíæ From cache: 2,500 (50.0%)\n",
            "   üÜï Newly generated: 2,500 (50.0%)\n",
            "   ‚ùå Failed: 0 (0.0%)\n",
            "\n",
            "üöÄ PERFORMANCE COMPARISON:\n",
            "============================================================\n",
            "   üìä Naive sequential approach: ~0.6 hours\n",
            "   ‚ö° LLM Batch Helper: 0.6 minutes\n",
            "   üéØ SPEEDUP: 63x FASTER!\n",
            "   üí∞ Time saved: 0.6 hours\n",
            "\n",
            "üíæ CACHING EFFECTIVENESS:\n",
            "============================================================\n",
            "   üìä Cache hit rate: 50.0%\n",
            "   ‚ö° Only processed 2,500 new prompts out of 5,000\n",
            "   üí° This means you can interrupt and resume without losing work!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Configuration for large scale processing\n",
        "large_config = LLMConfig(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=1.0,\n",
        "    max_completion_tokens=50,\n",
        "    max_concurrent_requests=500,  # Much higher concurrency!\n",
        "    max_retries=3\n",
        ")\n",
        "\n",
        "# Process all 5K prompts\n",
        "all_prompts = test_prompts  # All 5,000 prompts\n",
        "\n",
        "print(f\"üéØ Processing ALL {len(all_prompts):,} prompts!\")\n",
        "print(f\"üíæ First {len(medium_prompts):,} should be served from cache\")\n",
        "print(f\"üöÄ Only {len(all_prompts) - len(medium_prompts):,} new prompts need processing\")\n",
        "print(f\"‚ö° Using {large_config.max_concurrent_requests} concurrent requests\")\n",
        "\n",
        "start_time = time.time()\n",
        "large_results = process_prompts_batch(\n",
        "    config=large_config,\n",
        "    provider=\"openai\",\n",
        "    prompts=all_prompts,\n",
        "    cache_dir=\"performance_cache\",\n",
        "    desc=f\"Large Scale ({len(all_prompts):,} prompts)\"\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Comprehensive analysis\n",
        "successful = sum(1 for r in large_results.values() if \"error\" not in r)\n",
        "cached = sum(1 for r in large_results.values() if r.get(\"from_cache\", False))\n",
        "generated = successful - cached\n",
        "failed = len(large_results) - successful\n",
        "\n",
        "print(f\"\\nüèÜ FINAL RESULTS - Large Scale Processing:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"   üìä Total prompts: {len(all_prompts):,}\")\n",
        "print(f\"   ‚è±Ô∏è  Total time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
        "print(f\"   üìà Average time per prompt: {elapsed_time/len(all_prompts):.4f} seconds\")\n",
        "print(f\"\\nüìã Response Breakdown:\")\n",
        "print(f\"   ‚úÖ Successful: {successful:,} ({successful/len(all_prompts)*100:.1f}%)\")\n",
        "print(f\"   üíæ From cache: {cached:,} ({cached/len(all_prompts)*100:.1f}%)\")\n",
        "print(f\"   üÜï Newly generated: {generated:,} ({generated/len(all_prompts)*100:.1f}%)\")\n",
        "print(f\"   ‚ùå Failed: {failed:,} ({failed/len(all_prompts)*100:.1f}%)\")\n",
        "\n",
        "# Ultimate performance comparison\n",
        "print(f\"\\nüöÄ PERFORMANCE COMPARISON:\")\n",
        "print(f\"{'='*60}\")\n",
        "speedup = naive_estimated_time / elapsed_time\n",
        "print(f\"   üìä Naive sequential approach: ~{naive_estimated_time/3600:.1f} hours\")\n",
        "print(f\"   ‚ö° LLM Batch Helper: {elapsed_time/60:.1f} minutes\")\n",
        "print(f\"   üéØ SPEEDUP: {speedup:.0f}x FASTER!\")\n",
        "print(f\"   üí∞ Time saved: {(naive_estimated_time - elapsed_time)/3600:.1f} hours\")\n",
        "\n",
        "# Caching effectiveness\n",
        "print(f\"\\nüíæ CACHING EFFECTIVENESS:\")\n",
        "print(f\"{'='*60}\")\n",
        "cache_hit_rate = cached / len(all_prompts) * 100\n",
        "print(f\"   üìä Cache hit rate: {cache_hit_rate:.1f}%\")\n",
        "print(f\"   ‚ö° Only processed {generated:,} new prompts out of {len(all_prompts):,}\")\n",
        "print(f\"   üí° This means you can interrupt and resume without losing work!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've seen the power of LLM Batch Helper, here are some ways to apply it to your own projects:\n",
        "\n",
        "### üöÄ For Your Own Projects:\n",
        "\n",
        "1. **Start Small**: Always test with 10-50 examples first\n",
        "2. **Find Your Sweet Spot**: Experiment with `max_concurrent_requests` (start with 50-100)\n",
        "3. **Use Caching**: Set up a dedicated cache directory for each project\n",
        "4. **Monitor and Tune**: Watch for rate limits and adjust concurrency accordingly\n",
        "5. **Add Verification**: Use custom verification callbacks for quality control\n",
        "\n",
        "### üìö Learn More:\n",
        "\n",
        "- Check out the main tutorial notebook for more features\n",
        "- Read the documentation for advanced configuration options\n",
        "- Explore different providers (OpenAI, OpenRouter, Together.ai)\n",
        "- Try custom verification callbacks for your use cases\n",
        "\n",
        "### üí° Common Use Cases:\n",
        "\n",
        "- **Data Labeling**: Process thousands of examples for ML training\n",
        "- **Content Generation**: Create large amounts of content efficiently  \n",
        "- **Evaluation**: Run LLM-as-a-judge on large datasets\n",
        "- **Research**: Conduct large-scale experiments with different prompts\n",
        "- **Simulation**: Run multi-agent conversations or scenarios\n",
        "\n",
        "Happy batch processing! üéØ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-batch-helper-cXcqr_wg-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

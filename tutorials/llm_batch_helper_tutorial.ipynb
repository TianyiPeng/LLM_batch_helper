{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LLM Batch Helper Tutorial\n",
        "\n",
        "Welcome to the comprehensive tutorial for the LLM Batch Helper package! This notebook will walk you through all the features and capabilities of the package.\n",
        "\n",
        "## What is LLM Batch Helper?\n",
        "\n",
        "LLM Batch Helper is a Python package that enables efficient batch submission of prompts to LLM APIs with:\n",
        "- **Simplified API** - No async/await syntax needed! üéâ\n",
        "- **Async processing** handled implicitly for faster execution\n",
        "- **Response caching** to avoid redundant API calls\n",
        "- **Multiple input formats** (lists, files)\n",
        "- **Custom verification** for response quality\n",
        "- **Retry logic** with exponential backoff and detailed logging\n",
        "- **Progress tracking** with progress bars\n",
        "- **Jupyter notebook support** - works seamlessly without event loop issues\n",
        "\n",
        "## üéâ New in v0.3.0\n",
        "\n",
        "- **Simplified Interface**: `process_prompts_batch()` now works without async/await\n",
        "- **Jupyter Compatible**: No more event loop management needed\n",
        "- **Detailed Retry Logging**: See exactly what happens during retries\n",
        "- **Backward Compatibility**: Original async API still available\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this tutorial, make sure you have:\n",
        "1. Installed the package: `pip install llm_batch_helper`\n",
        "2. Set up your OpenAI API key: `export OPENAI_API_KEY=\"your-api-key\"`\n",
        "\n",
        "Let's get started! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install llm_batch_helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OpenAI API key is set!\n",
            "üéâ LLM Batch Helper imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Import the LLM Batch Helper components\n",
        "from llm_batch_helper import LLMConfig, process_prompts_batch, LLMCache\n",
        "\n",
        "# Check if OpenAI API key is set\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found!\")\n",
        "    print(\"Please set your API key: export OPENAI_API_KEY='your-api-key'\")\n",
        "else:\n",
        "    print(\"‚úÖ OpenAI API key is set!\")\n",
        "    \n",
        "print(\"üéâ Note: No more asyncio imports needed - the API is now simplified!\")\n",
        "    \n",
        "print(\"üéâ LLM Batch Helper imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üÜï What's New in v0.3.0\n",
        "\n",
        "This tutorial has been updated to showcase the **simplified API**! Here's what changed:\n",
        "\n",
        "### ‚úÖ Before (v0.2.0 and earlier)\n",
        "```python\n",
        "import asyncio\n",
        "from llm_batch_helper import process_prompts_batch\n",
        "\n",
        "async def main():\n",
        "    results = await process_prompts_batch(...)\n",
        "    return results\n",
        "\n",
        "results = asyncio.run(main())\n",
        "```\n",
        "\n",
        "### üéâ After (v0.3.0+)\n",
        "```python\n",
        "from llm_batch_helper import process_prompts_batch\n",
        "\n",
        "# That's it - no async/await needed!\n",
        "results = process_prompts_batch(...)\n",
        "```\n",
        "\n",
        "### üîç New Retry Logging\n",
        "You'll also see detailed logging when retries happen:\n",
        "```\n",
        "üîÑ [14:23:15] Retry attempt 1/5:\n",
        "   Error: AuthenticationError (status: 401)\n",
        "   Message: Incorrect API key provided...\n",
        "   Waiting 4.0s before next attempt...\n",
        "```\n",
        "\n",
        "Let's dive into the tutorial! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 1. Basic Usage\n",
        "\n",
        "Let's start with a simple example: processing a list of prompts using the OpenAI API.\n",
        "\n",
        "## Configuration\n",
        "\n",
        "First, we need to create an `LLMConfig` object that defines how we want to interact with the API:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration created!\n",
            "Model: gpt-4o-mini\n",
            "Temperature: 0.7\n",
            "Max tokens: 100\n",
            "Max concurrent requests: 5\n"
          ]
        }
      ],
      "source": [
        "# Create a basic configuration\n",
        "config = LLMConfig(\n",
        "    model_name=\"gpt-4o-mini\",          # OpenAI model to use\n",
        "    temperature=1.0,                   # Controls randomness (0.0 = deterministic, 1.0 = very random)\n",
        "    max_completion_tokens=100,         # Maximum tokens in response (preferred over max_tokens)\n",
        "    max_concurrent_requests=5,         # Number of parallel requests\n",
        "    max_retries=3                      # Number of retries on failure\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuration created!\")\n",
        "print(f\"Model: {config.model_name}\")\n",
        "print(f\"Temperature: {config.temperature}\")\n",
        "print(f\"Max tokens: {config.max_tokens}\")\n",
        "print(f\"Max concurrent requests: {config.max_concurrent_requests}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Processing 5 prompts...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Basic Example: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Processed 5 prompts!\n",
            "üìã Results:\n",
            "[GENERATED] 87867a835fb23e91329cee573f1d5aef: The capital of France is Paris.\n",
            "[GENERATED] fcf08bf32b9d323b77ffabe22d9b0ff3: 'Romeo and Juliet' was written by William Shakespeare.\n",
            "[GENERATED] 6125526fef186f25b943810b334fe44a: The largest planet in our solar system is Jupiter.\n",
            "[GENERATED] 99fe35a00746182c5f4aac010ebaff4b: 2 + 2 = 4.\n",
            "[GENERATED] f81541ba5472630e22211367b36be7f6: I would need to see the tutorial or have information about it to determine the programming language it is written for. Please provide more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a list of prompts to process\n",
        "prompts = [\n",
        "    \"What is the capital of France? Answer briefly.\",\n",
        "    \"What is 2+2? Answer briefly.\",\n",
        "    \"Who wrote 'Romeo and Juliet'? Answer briefly.\",\n",
        "    \"What is the largest planet in our solar system? Answer briefly.\",\n",
        "    \"What programming language is this tutorial written for? Answer briefly.\"\n",
        "]\n",
        "\n",
        "print(f\"üìù Processing {len(prompts)} prompts...\")\n",
        "\n",
        "# üéâ NEW: Process the prompts with simplified API - no async/await needed!\n",
        "results = process_prompts_batch(\n",
        "    config=config,\n",
        "    provider=\"openai\",\n",
        "    prompts=prompts,\n",
        "    cache_dir=\"tutorial_cache\",\n",
        "    desc=\"Basic Example\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Processed {len(results)} prompts!\")\n",
        "print(\"üìã Results:\")\n",
        "for prompt_id, response in results.items():\n",
        "    status = \"[CACHE]\" if response.get(\"from_cache\") else \"[GENERATED]\"\n",
        "    if \"error\" in response:\n",
        "        print(f\"{status} {prompt_id}: ERROR - {response['error']}\")\n",
        "    else:\n",
        "        print(f\"{status} {prompt_id}: {response['response_text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 2. File-Based Processing\n",
        "\n",
        "Instead of defining prompts in code, you can organize them in text files. This is especially useful for:\n",
        "- Large sets of prompts\n",
        "- Collaborative prompt development\n",
        "- Version control of prompts\n",
        "- Easier prompt management\n",
        "\n",
        "Let's create some example prompt files and process them:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Created 5 prompt files in 'tutorial_prompts'\n",
            "üìã Files created:\n",
            "  - science_fact.txt\n",
            "  - creative_writing.txt\n",
            "  - problem_solving.txt\n",
            "  - philosophy.txt\n",
            "  - technology.txt\n",
            "\n",
            "üìÇ Files in tutorial_prompts:\n",
            "  - philosophy.txt\n",
            "  - creative_writing.txt\n",
            "  - problem_solving.txt\n",
            "  - technology.txt\n",
            "  - science_fact.txt\n"
          ]
        }
      ],
      "source": [
        "# Create a directory for prompt files\n",
        "prompts_dir = Path(\"tutorial_prompts\")\n",
        "prompts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create example prompt files\n",
        "prompt_files = {\n",
        "    \"science_fact\": \"Tell me an interesting fact about black holes. Keep it under 50 words.\",\n",
        "    \"creative_writing\": \"Write a short poem about coding. Make it fun and rhyming.\",\n",
        "    \"problem_solving\": \"Explain how to solve a Rubik's cube in 3 simple steps.\",\n",
        "    \"philosophy\": \"What is the meaning of life according to different philosophical traditions?\",\n",
        "    \"technology\": \"Explain quantum computing to a 10-year-old.\"\n",
        "}\n",
        "\n",
        "# Write prompts to files\n",
        "for filename, content in prompt_files.items():\n",
        "    filepath = prompts_dir / f\"{filename}.txt\"\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "print(f\"üìÅ Created {len(prompt_files)} prompt files in '{prompts_dir}'\")\n",
        "print(\"üìã Files created:\")\n",
        "for filename in prompt_files.keys():\n",
        "    print(f\"  - {filename}.txt\")\n",
        "    \n",
        "# List the actual files created\n",
        "print(f\"\\nüìÇ Files in {prompts_dir}:\")\n",
        "for file in prompts_dir.glob(\"*.txt\"):\n",
        "    print(f\"  - {file.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File-based Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:03<00:00,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Processed 5 files!\n",
            "üìã Results:\n",
            "[GENERATED] science_fact: Black holes can warp time and space due to their immense gravity. Near a black hole, time slows down...\n",
            "[GENERATED] technology: Sure! Imagine you have a really big box of LEGO bricks. Each brick can be either red or blue, and yo...\n",
            "[GENERATED] creative_writing: In a world of zeros, ones, and code,  \n",
            "A coder‚Äôs journey, a winding road.  \n",
            "With fingers flying, key...\n",
            "[GENERATED] problem_solving: Solving a Rubik's Cube can be complex, but here's a simplified approach broken down into three essen...\n",
            "[GENERATED] philosophy: The meaning of life has been a central question in philosophy, and various traditions offer differen...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process the prompt files with simplified API\n",
        "file_results = process_prompts_batch(\n",
        "    config=config,\n",
        "    provider=\"openai\",\n",
        "    input_dir=str(prompts_dir),  # Directory containing .txt files\n",
        "    cache_dir=\"tutorial_cache\",\n",
        "    desc=\"File-based Processing\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Processed {len(file_results)} files!\")\n",
        "print(\"üìã Results:\")\n",
        "for prompt_id, response in file_results.items():\n",
        "    status = \"[CACHE]\" if response.get(\"from_cache\") else \"[GENERATED]\"\n",
        "    if \"error\" in response:\n",
        "        print(f\"{status} {prompt_id}: ERROR - {response['error']}\")\n",
        "    else:\n",
        "        print(f\"{status} {prompt_id}: {response['response_text'][:100]}...\")  # Show first 100 chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 3. Custom Verification\n",
        "\n",
        "Sometimes you want to ensure the quality of responses before accepting them. The LLM Batch Helper supports custom verification functions, for example, you can check:\n",
        "\n",
        "- Check response length\n",
        "- Validate content format\n",
        "- Ensure specific keywords are present/absent\n",
        "- Implement custom quality checks\n",
        "\n",
        "If verification fails, the system will retry with a new request. Let's create a custom verification function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration with custom verification created!\n"
          ]
        }
      ],
      "source": [
        "# Define a custom verification function\n",
        "def quality_verification(prompt_id, llm_response_data, original_prompt_text, **kwargs):\n",
        "    \"\"\"\n",
        "    Custom verification function that checks:\n",
        "    1. Response is not empty\n",
        "    2. Response meets minimum length requirement\n",
        "    \"\"\"\n",
        "    \n",
        "    response_text = llm_response_data.get(\"response_text\", \"\")\n",
        "    \n",
        "    # Check 1: Not empty\n",
        "    if not response_text.strip():\n",
        "        print(f\"‚ùå {prompt_id}: Empty response\")\n",
        "        return False\n",
        "    \n",
        "    # Check 2: Minimum length\n",
        "    min_length = kwargs.get(\"min_length\", 20)\n",
        "    if len(response_text) < min_length:\n",
        "        print(f\"‚ùå {prompt_id}: Response too short ({len(response_text)} < {min_length})\")\n",
        "        return False\n",
        "    \n",
        "    print(f\"‚úÖ {prompt_id}: Verification passed\")\n",
        "    return True\n",
        "\n",
        "# Create config with custom verification\n",
        "verified_config = LLMConfig(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=1.0,\n",
        "    max_completion_tokens=150,\n",
        "    max_concurrent_requests=3,\n",
        "    max_retries=3,\n",
        "    verification_callback=quality_verification,\n",
        "    verification_callback_args={\"min_length\": 30}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuration with custom verification created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Testing custom verification...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verification Example:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ 215832376ebe50e1f3fc8d5417ab3cb9: Verification passed\n",
            "‚úÖ de61d1e78c4495cad5c9281947620db4: Verification passed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verification Example: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ a50c47dd8f661a6b85fbd251004c986b: Verification passed\n",
            "\n",
            "‚úÖ Verification test completed!\n",
            "üìã Results:\n",
            "[CACHE] 215832376ebe50e1f3fc8d5417ab3cb9: The square root of 64 is 8. \n",
            "\n",
            "To calculate the square root, you can think of it ...\n",
            "[CACHE] de61d1e78c4495cad5c9281947620db4: Photosynthesis is a biochemical process through which green plants, algae, and s...\n",
            "[GENERATED] a50c47dd8f661a6b85fbd251004c986b: Sure! Machine learning is a way for computers to learn from data and make decisi...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test prompts for verification\n",
        "verification_test_prompts = [\n",
        "    \"What is the square root of 64? Explain how you calculated it.\",\n",
        "    \"Write a detailed explanation of how photosynthesis works.\",\n",
        "    \"Describe the process of machine learning in simple terms.\",\n",
        "]\n",
        "\n",
        "# Process with verification using simplified API\n",
        "print(\"üîç Testing custom verification...\")\n",
        "verification_results = process_prompts_batch(\n",
        "    config=verified_config,\n",
        "    provider=\"openai\",\n",
        "    prompts=verification_test_prompts,\n",
        "    cache_dir=\"tutorial_cache\",\n",
        "    desc=\"Verification Example\"\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Verification test completed!\")\n",
        "print(\"üìã Results:\")\n",
        "for prompt_id, response in verification_results.items():\n",
        "    status = \"[CACHE]\" if response.get(\"from_cache\") else \"[GENERATED]\"\n",
        "    if \"error\" in response:\n",
        "        print(f\"{status} {prompt_id}: ERROR - {response['error']}\")\n",
        "    else:\n",
        "        print(f\"{status} {prompt_id}: {response['response_text'][:80]}...\")  # Show first 80 chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 4. Caching Features\n",
        "\n",
        "One of the most powerful features of LLM Batch Helper is automatic response caching. This:\n",
        "\n",
        "- **Saves money** by avoiding redundant API calls\n",
        "- **Improves speed** by reusing previous responses\n",
        "- **Enables experimentation** without re-running expensive operations\n",
        "- **Supports reproducibility** in research and development\n",
        "\n",
        "Let's explore the caching functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the cache directory\n",
        "cache_dir = Path(\"tutorial_cache\")\n",
        "print(f\"üìÅ Cache directory: {cache_dir}\")\n",
        "print(f\"üìÇ Cache exists: {cache_dir.exists()}\")\n",
        "\n",
        "if cache_dir.exists():\n",
        "    cache_files = list(cache_dir.glob(\"*.json\"))\n",
        "    print(f\"üìÑ Number of cached responses: {len(cache_files)}\")\n",
        "    \n",
        "    # Show some cache files\n",
        "    print(\"\\nüìã Cached responses:\")\n",
        "    for i, cache_file in enumerate(cache_files[:5]):  # Show first 5\n",
        "        print(f\"  {i+1}. {cache_file.name}\")\n",
        "    \n",
        "    if len(cache_files) > 5:\n",
        "        print(f\"  ... and {len(cache_files) - 5} more\")\n",
        "    \n",
        "    # Show content of one cache file\n",
        "    if cache_files:\n",
        "        sample_file = cache_files[0]\n",
        "        print(f\"\\nüìÑ Sample cache file ({sample_file.name}):\")\n",
        "        with open(sample_file, 'r') as f:\n",
        "            cache_content = json.load(f)\n",
        "            print(f\"  Prompt: {cache_content.get('prompt_input', 'N/A')[:60]}...\")\n",
        "            print(f\"  Response: {cache_content.get('llm_response', {}).get('response_text', 'N/A')[:60]}...\")\n",
        "            print(f\"  Cached at: {cache_content.get('llm_response', {}).get('timestamp', 'N/A')}\")\n",
        "else:\n",
        "    print(\"üîç No cache directory found yet - it will be created when you run examples!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Cache Management Examples:\n",
            "‚úÖ Found cached response for prompt ID: 87867a835fb23e91329cee573f1d5aef\n",
            "   Response: The capital of France is Paris.\n",
            "\n",
            "üõ†Ô∏è  Cache Operations:\n",
            "   - Cache directory: tutorial_cache\n",
            "   - Cache exists: True\n",
            "üìä Cache Statistics:\n",
            "   - Files: 13\n",
            "   - Total size: 7.59 KB\n",
            "   - Average file size: 0.58 KB\n"
          ]
        }
      ],
      "source": [
        "import hashlib\n",
        "# Cache Management\n",
        "cache = LLMCache(cache_dir=\"tutorial_cache\")\n",
        "\n",
        "# Demonstrate cache usage\n",
        "print(\"üîß Cache Management Examples:\")\n",
        "\n",
        "# Check if a specific prompt is cached\n",
        "sample_prompt = \"What is the capital of France? Answer briefly.\"\n",
        "prompt_id = hashlib.sha256(sample_prompt.encode()).hexdigest()[:32]\n",
        "\n",
        "# Try to get cached response\n",
        "cached_response = cache.get_cached_response(prompt_id)\n",
        "if cached_response:\n",
        "    print(f\"‚úÖ Found cached response for prompt ID: {prompt_id}\")\n",
        "    print(f\"   Response: {cached_response['llm_response']['response_text']}\")\n",
        "else:\n",
        "    print(f\"‚ùå No cached response found for prompt ID: {prompt_id}\")\n",
        "\n",
        "# Cache operations\n",
        "print(f\"\\nüõ†Ô∏è  Cache Operations:\")\n",
        "print(f\"   - Cache directory: {cache.cache_dir}\")\n",
        "print(f\"   - Cache exists: {cache.cache_dir.exists()}\")\n",
        "\n",
        "# Option to clear cache (uncomment if needed)\n",
        "# cache.clear_cache()\n",
        "# print(\"üóëÔ∏è  Cache cleared!\")\n",
        "\n",
        "# Show cache statistics\n",
        "if cache.cache_dir.exists():\n",
        "    cache_files = list(cache.cache_dir.glob(\"*.json\"))\n",
        "    total_size = sum(f.stat().st_size for f in cache_files)\n",
        "    print(f\"üìä Cache Statistics:\")\n",
        "    print(f\"   - Files: {len(cache_files)}\")\n",
        "    print(f\"   - Total size: {total_size / 1024:.2f} KB\")\n",
        "    print(f\"   - Average file size: {total_size / len(cache_files) / 1024:.2f} KB\" if cache_files else \"   - Average file size: 0 KB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm-batch-helper-cXcqr_wg-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
